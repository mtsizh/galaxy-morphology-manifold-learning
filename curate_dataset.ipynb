{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/mtsizh/galaxy-morphology-manifold-learning/blob/main/curate_dataset.ipynb",
      "authorship_tag": "ABX9TyOSk7NyOeupUcWq4t4/1xBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtsizh/galaxy-morphology-manifold-learning/blob/main/curate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below downloads `gz_decals_volunteers_5.parquet` dataset and curates. We use only these datapoints where there were enough answers and high confidence."
      ],
      "metadata": {
        "id": "grURRklstXjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and curate table\n",
        "\n",
        "from functools import partial\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print('DOWNLOADING TABLE')\n",
        "!if [ ! -f \"gz_decals_volunteers_5.parquet\" ]; then wget https://zenodo.org/records/4573248/files/gz_decals_volunteers_5.parquet; fi\n",
        "\n",
        "df = pd.read_parquet('gz_decals_volunteers_5.parquet')\n",
        "working_columns = ['iauname',\n",
        "                   'png_loc',\n",
        "                   'smooth-or-featured_smooth_fraction',\n",
        "                   'smooth-or-featured_total-votes',\n",
        "                   'has-spiral-arms_yes_fraction',\n",
        "                   'has-spiral-arms_total-votes',\n",
        "                   'how-rounded_total-votes',\n",
        "                   'how-rounded_round_fraction',\n",
        "                   'how-rounded_in-between_fraction',\n",
        "                   'how-rounded_cigar-shaped_fraction',\n",
        "                   'disk-edge-on_total-votes',\n",
        "                   'disk-edge-on_yes_fraction',\n",
        "                   'disk-edge-on_no_fraction']\n",
        "\n",
        "\n",
        "df = df[working_columns] # remove unnecessary columns\n",
        "df['class'] = \"\" # add class column\n",
        "\n",
        "lt = lambda x : np.isfinite and partial(operator.gt, x)\n",
        "gt = lambda x : np.isfinite and partial(operator.lt, x)\n",
        "\n",
        "conditions = {\n",
        "    'has spiral arms': {'has-spiral-arms_yes_fraction': gt(0.8),\n",
        "                        'has-spiral-arms_total-votes': gt(15)},\n",
        "    'no spiral arms': {'has-spiral-arms_yes_fraction': lt(0.2),\n",
        "                       'has-spiral-arms_total-votes': gt(15)},\n",
        "    'round': {'how-rounded_round_fraction': gt(0.8),\n",
        "              'how-rounded_total-votes': gt(10)},\n",
        "    'inbetween': {'how-rounded_in-between_fraction': gt(0.8),\n",
        "                  'how-rounded_total-votes': gt(10)},\n",
        "    'cigar': {'how-rounded_cigar-shaped_fraction': gt(0.8),\n",
        "              'how-rounded_total-votes': gt(10)},\n",
        "    'edge on': {'disk-edge-on_yes_fraction': gt(0.8),\n",
        "                'disk-edge-on_total-votes': gt(10)},\n",
        "    'edge off': {'disk-edge-on_no_fraction': gt(0.8),\n",
        "                 'disk-edge-on_total-votes': gt(10)},\n",
        "    'smooth': {'smooth-or-featured_smooth_fraction': gt(0.8),\n",
        "               'smooth-or-featured_total-votes': gt(10)},\n",
        "    'featured': {'smooth-or-featured_smooth_fraction': lt(0.2),\n",
        "                 'smooth-or-featured_total-votes': gt(10)}\n",
        "}\n",
        "\n",
        "for class_name, cond_rules in conditions.items():\n",
        "  bool_arr = np.ones(len(df), dtype=bool)\n",
        "  for col, condition_func in cond_rules.items():\n",
        "    bool_arr &= condition_func(df[col])\n",
        "  df.loc[bool_arr, 'class'] += f', {class_name}'\n",
        "\n",
        "df['class'] = df['class'].str.lstrip(',')\n",
        "df = df[df['class'] != \"\"] #remove unclassified\n",
        "\n",
        "print('SAVING CURATED TABLE')\n",
        "df.to_parquet('curated_dataset.parquet', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMZYjIHikF5M",
        "outputId": "b1ad99b6-9e60-4760-d5fb-1d540b049453",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DOWNLOADING TABLE\n",
            "--2025-02-25 11:36:08--  https://zenodo.org/records/4573248/files/gz_decals_volunteers_5.parquet\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.48.194, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40528939 (39M) [application/octet-stream]\n",
            "Saving to: ‘gz_decals_volunteers_5.parquet’\n",
            "\n",
            "gz_decals_volunteer 100%[===================>]  38.65M   454KB/s    in 89s     \n",
            "\n",
            "2025-02-25 11:37:38 (446 KB/s) - ‘gz_decals_volunteers_5.parquet’ saved [40528939/40528939]\n",
            "\n",
            "SAVING CURATED TABLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and curating images based on curated table. Please note, the procedure is quite slow (downloading 4 parts x 20 min per part). Images are cropped to 120 x 120 central part. The result is archived in case you want to download it (final size will be approximately 400 Mb, the full original dataset is 4 parts x 20 Gb each)."
      ],
      "metadata": {
        "id": "Jj5CXfAVtuEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and curate images\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import pathlib\n",
        "\n",
        "df = pd.read_parquet('curated_dataset.parquet')\n",
        "all_files = df['png_loc'].tolist()\n",
        "all_files = [str(pathlib.Path(*pathlib.Path(f).parts[1:])) for f in all_files]\n",
        "\n",
        "save_to_dir = 'curated_imgs'\n",
        "save_large_dir = 'curated_imgs_large'\n",
        "if not os.path.exists(save_to_dir):\n",
        "  os.makedirs(save_to_dir, exist_ok=True)\n",
        "if not os.path.exists(save_large_dir):\n",
        "  os.makedirs(save_large_dir, exist_ok=True)\n",
        "\n",
        "parts_urls = [\n",
        "    'https://zenodo.org/records/4573248/files/gz_decals_dr5_png_part1.zip',\n",
        "    'https://zenodo.org/records/4573248/files/gz_decals_dr5_png_part2.zip',\n",
        "    'https://zenodo.org/records/4573248/files/gz_decals_dr5_png_part3.zip',\n",
        "    'https://zenodo.org/records/4573248/files/gz_decals_dr5_png_part4.zip'\n",
        "]\n",
        "\n",
        "for part_idx, url in enumerate(parts_urls):\n",
        "  print(f'PART {part_idx+1} of {len(parts_urls)}')\n",
        "  _, filename = os.path.split(url)\n",
        "  if not os.path.isfile(filename):\n",
        "    !wget {url}\n",
        "  if not os.path.isfile(filename):\n",
        "    print(f'ERROR LOADING PART {part_idx+1}')\n",
        "    continue\n",
        "  print(f'UNZIPPING {filename}')\n",
        "  with zipfile.ZipFile(filename) as z:\n",
        "    files_in_part = list(set(all_files) & set(z.namelist()))\n",
        "    with tqdm(total=len(files_in_part)) as progress:\n",
        "      for filename1 in files_in_part:\n",
        "        full_path = os.path.join(save_large_dir, filename1)\n",
        "        path = pathlib.Path(full_path)\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(full_path, 'wb') as f:\n",
        "          f.write(z.read(filename1))\n",
        "        progress.update()\n",
        "  print('FILES EXTRACTED')\n",
        "  !rm {filename}\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# Define the cropping function\n",
        "def crop_center(image, crop_width, crop_height):\n",
        "    width, height = image.size\n",
        "    left = (width - crop_width) // 2\n",
        "    top = (height - crop_height) // 2\n",
        "    right = left + crop_width\n",
        "    bottom = top + crop_height\n",
        "    return image.crop((left, top, right, bottom))\n",
        "\n",
        "def count_files(folder):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(folder):\n",
        "        count += len(files)\n",
        "    return count\n",
        "\n",
        "print('CROPPING IMAGES to 120x120')\n",
        "with tqdm(total=count_files(save_large_dir)) as progress:\n",
        "  for root, _, files in os.walk(save_large_dir):\n",
        "      for file in files:\n",
        "          file_path = os.path.join(root, file)\n",
        "          save_to = os.path.join(save_to_dir, file)\n",
        "          with Image.open(file_path) as img:\n",
        "            gray_img = img.convert(\"L\")\n",
        "            cropped_img = crop_center(gray_img, 120, 120)\n",
        "            cropped_img.save(save_to)\n",
        "            progress.update()\n",
        "\n",
        "print('ZIPPING just in case')\n",
        "!zip -q -r curated_imgs.zip curated_imgs curated_dataset.parquet && echo \"[ZIPPED] cropped images\" || echo \"[ERROR] zipping cropped images\"\n",
        "!zip -q -r curated_imgs_large.zip curated_imgs_large curated_dataset.parquet && echo \"[ZIPPED] large images\" || echo \"[ERROR] zipping large images\"\n",
        "\n",
        "\n",
        "print('PROCESSING COMPLETE!')"
      ],
      "metadata": {
        "id": "jafOHN4NN41E",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two archives should be generated\n",
        "\n",
        "|Archive|image size|color|used for|expected size|\n",
        "|---|---|---|---|---|\n",
        "|curated_imgs.zip|120x120|grayscale|dimensionality reduction|380Mb|\n",
        "|curated_imgs_large.zip|?|RGB|Petrosian radii calculation|??|\n",
        "\n",
        "You can connect GoogleDrive and store archive with images for future purposes to avoid the whole process of downloading and curating the data"
      ],
      "metadata": {
        "id": "JfS2Lrsc5jrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp curated_imgs.zip ./drive/MyDrive/\n",
        "!cp curated_imgs_large.zip ./drive/MyDrive/"
      ],
      "metadata": {
        "id": "QyG3cV-NIk8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or you can download the data if you wish"
      ],
      "metadata": {
        "id": "C16-k4d8WcvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('curated_imgs.zip')"
      ],
      "metadata": {
        "id": "wfJU7GeuWkRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}